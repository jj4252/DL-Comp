# @package _global_
# Default configuration
defaults:
  - model: dino_v2
  - data@data.0: fall2025_deeplearning
  - data@data.1: open_images
  - data@data.2: na_birds
  - optimizer: adamw
  - scheduler: cosine
  - evaluation/data@evaluation.data.0: cub200
  - evaluation/data@evaluation.data.1: mini_imagenet
  - evaluation/data@evaluation.data.2: sun397
  - _self_

# Experiment settings
experiment_name: ssl_vision_training
seed: 42

# Training settings
training:
  num_epochs: 250
  batch_size: 256 # batch size per GPU (optimized for ViT-Small, 2 A100 GPUs)
  num_workers: 16  # optimized for 2 multi-GPU trainnig (2 processes per GPU -> spawn 16 for each process -> total 32 worker processes)
  pin_memory: true
  gradient_accumulation_steps: 1
  mixed_precision: true
  gradient_clip: 3.0
  prefetch_factor: 2  # Prefetch more batches (2x default) to keep GPU fed
  persistent_workers: true  # Keep workers alive between epochs

# Distributed training
distributed:
  enabled: false         # Set to true when launching with torchrun on multiple GPUs
  backend: nccl
  init_method: env://
  find_unused_parameters: false

# Checkpointing
checkpoint:
  save_dir: /gpfs/data/fieremanslab/dayne/projects/DL-Final-Competition/checkpoints  # Base directory (experiment_name is appended)
  save_frequency: 5 # every n epochs
  resume_from: null  # null (fresh start) or full path to checkpoint from previous experiment

early_stopping:
  # Note: With knn_eval_frequency=10, patience=3 means 30 epochs without improvement
  patience: 3
  min_delta: 0.003  # at least 0.3%p improvement

# Logging
logging:
  log_dir: ./logs
  log_frequency: 100

# K-NN Evaluation
evaluation:
  batch_size: 256
  knn_eval_frequency: 10
  top_k_models: 5
  knn_k: 20
